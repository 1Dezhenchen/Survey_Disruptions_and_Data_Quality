---
title: "The Impact of Interview Disruptions on Data Quality in Social Surveys"
subtitle: "Analyzing How Various Interferences Affect Respondent Accuracy and Survey Reliability"
author: 
  - Dezhen Chen
thanks: "Code and data are available at: https://github.com/1Dezhenchen/The-Influence-of-Interview-Disruptions-on-Respondent-Data-Quality-in-Social-Surveys."
date: today
date-format: long
abstract: "In social surveys, disruptions during interviews can significantly impact the quality of respondent data, which in turn affects the reliability of the survey results. This study uses Bayesian hierarchical logistic regression to analyze how various factors—such as the type of disruption, the presence of different family members, interview conduct method, interviewer characteristics, and respondent attitudes—affect the accuracy and quality of responses. Our analysis shows that different types of disruptions have distinct impacts on the quality of responses, revealing critical insights into optimizing survey conditions to improve data reliability and validity."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(arrow)
library(knitr)
library(rstanarm)
library(tidyverse)
library(ggplot2)
library(bayesplot)
library(ggridges)
library(broom.mixed)
library(kableExtra)
library(car)
library(loo)
library(modelsummary)
library(caret)
```

# Introduction {#sec-intro}

In recent years, the quality of social survey data has become an important topic of inquiry, especially with regard to how different disruptions impact respondent behavior. Several studies have contributed to this body of knowledge by focusing on factors that may affect the reliability of survey responses. Groves et al. [@groves2011survey] examined the role of interviewer characteristics—such as experience and demeanor—in shaping the quality of responses through a multi-level modeling approach. In another key study, Couper and de Leeuw [@massey2013we] investigated how different modes of conducting interviews, like in-person versus telephone, influenced the comfort and attentiveness of respondents. Fowler and Mangione [@fowler1990standardized] took a different approach by analyzing observational data to assess the impact of third-party presence during interviews. Although these works laid a valuable foundation for understanding data quality, there remains a gap in systematically analyzing the distinct effects of various kinds of disruptions, such as familial or environmental interference, on respondent understanding and response accuracy.

This study endeavors to address these overlooked aspects by closely examining the role of interview disruptions on data quality in a major social survey. Specifically, our analysis explores the influence of different types of interferences—including interruptions from partners, children, and other family members—on respondents' comprehension and engagement during the interview process. We employed a Bayesian logistic regression model utilizing the rstanarm package in R to capture these influences. The model incorporates a range of predictor variables, such as interviewer characteristics (age and gender), different methods of interview administration, and family-related interferences. During data preparation, the categorical levels of several predictor variables were simplified, excluding the 'country' variable, to ensure interpretability and to streamline model estimation. We also included indicators of respondent attitudes, such as whether questions were clarified during the interview or if reluctance to answer was observed, aiming for a more comprehensive understanding of respondent engagement.

Our results reveal compelling insights into the effects of these disruptions on data quality. We found that family-related interferences, notably from partners and parents, were associated with reduced comprehension on the part of respondents. The findings also suggested that interviews conducted in-person resulted in significantly higher levels of respondent understanding compared to those conducted by telephone. Additionally, older interviewers generally achieved better data quality, which could reflect an ability to manage disruptions more effectively or employ better rapport-building techniques. The influence of the interviewer's gender was also noted, though it appeared to interact in more complex ways with other factors such as the mode of interview and the specific type of interference. Taken together, these findings underscore the importance of managing both environmental and personal factors during interviews to improve data accuracy.

Beyond merely quantifying these effects, our study provides deeper implications for enhancing data collection practices in the field. The evidence points to the need for improved interviewer training focused on managing disruptions effectively, which can help ensure the collection of high-quality data even in less-than-ideal circumstances. The significant negative effects of family interference suggest that in-home surveys could benefit from targeted strategies to minimize such disruptions, including adjusting the timing of interviews or providing incentives for privacy during responses. Furthermore, the results highlight the importance of understanding respondent reluctance and tailoring interviewer interactions to reduce its occurrence. These insights have relevance for not only improving the current practices of social surveys but also for shaping future methodologies in an increasingly diverse and complex interviewing landscape.

The paper is structured to offer an in-depth examination of the factors that affect respondent comprehension during social survey interviews. Following the introduction in @sec-intro, @sec-data provides a detailed overview of the dataset, focusing on the different variables and their role in assessing data quality. It also outlines the preprocessing steps taken, including data cleaning and the recoding of variables. @sec-model introduces the Bayesian logistic regression methodology, explaining the statistical approach and justifying the selection of priors for the analysis. In @sec-result, we present the empirical findings, including parameter estimates and an interpretation of the relationships uncovered by the model. @sec-discussion discusses the practical implications of these findings, emphasizing the improvements needed in interviewer training and the strategies for reducing respondent interference. Finally, @sec-model-details summarizes the key contributions of this research, outlines the limitations, and suggests avenues for future work—particularly concerning the integration of new data collection technologies and adaptive survey designs aimed at mitigating the impact of disruptions on respondent data quality.

# Estimand

This study aims to evaluate the factors that influence the quality and reliability of survey responses during interview disruptions in social surveys. By analyzing variables such as different types of interferences (partner, child, parent, and relative interferences), interview methodologies, the demographic characteristics of interviewers (such as age and gender), the language of the questionnaire, and respondent behaviors like reluctance, clarification requests, and effort put into answering, this research seeks to understand how these elements affect the consistency and clarity of responses. The core goal is to determine the impact of these factors on respondent understanding and data reliability, thereby providing insights into improving survey methods and reducing biases in data collection.

# Data {#sec-data}

## Overview

We use the statistical programming language R [@citeR] to process, analyze, and visualize survey data exploring the impact of interview disruptions on response quality. The dataset for this study is sourced from [European Social Survey](https://ess.sikt.no/en/datafile/0c3f5fda-4fe0-42f2-b4c8-4ef093c0508f/7?tab=0&elems=de8c08b6-e406-4c56-92ee-4272818cae96_1), which collects information on societal attitudes and behaviors across multiple European countries. It includes several subsets tailored to different stages of the analysis, allowing a systematic examination of factors affecting survey response quality. The main dataset, used for modeling, consists of 9,743 rows and 25 variables. These data points capture details about various types of disruptions (e.g., partner, child, or relative interference), interview methods, and respondent characteristics. All responses are collected using standardized methods to ensure comparability across countries, thereby facilitating a detailed exploration of the impact of environmental factors, such as interruptions during interviews, on response quality. 

To ensure the reliability of the analysis, we filtered the data to exclude incomplete or ambiguous responses for key variables, such as missing values in "respondent_understood." Responses were also recoded into binary categories to focus on meaningful distinctions in response quality, while variables with excessive categories were grouped to improve interpretability.Different countries may implement localized adaptations in survey administration, which could influence response behaviors. To account for this, country-level metadata is included, allowing for the analysis of cross-national variations. These preprocessing steps were essential to refine the dataset and facilitate robust modeling.

The following R packages were utilized to facilitate and streamline the data analysis process:

- **arrow** [@arrow]: Employed for efficiently reading and writing large datasets, enhancing data handling across different systems.

- **knitr** [@knitr]: Used to create dynamic documents with R, including reports and presentations. It enables seamless integration of R code and text.

- **rstanarm** [@rstanarm]: Provides tools for Bayesian regression modeling using the 'Stan' platform, making it straightforward to apply Bayesian models in a regression context.

- **tidyverse** [@tidyverse]: A collection of R packages used for data manipulation, visualization, and cleaning, providing a unified structure to handle complex data workflows.

- **ggplot2** [@ggplot2]: Utilized for creating detailed visualizations, offering a flexible grammar of graphics to develop sophisticated charts and graphs.

- **bayesplot** [@bayesplot]: Facilitates the visualization of Bayesian model results, useful for diagnostic checks and interpreting the results of Bayesian models.

- **ggridges** [@ggridges]: Used to create ridge plots, which are useful for visualizing distributions, enhancing the exploration of posterior distributions from Bayesian models.

- **broom.mixed** [@broom]: Extends the broom package to support models with random effects, summarizing mixed models in a tidy and interpretable format.

- **kableExtra** [@kableExtra]: Enhances the basic table formatting capabilities of the 'knitr' package, providing additional functions for creating aesthetically pleasing and informative tables.

- **car** [@car]: Used for regression diagnostics and statistical analysis, particularly valuable for testing assumptions and evaluating model quality.

- **loo** [@loo]: Provides tools for leave-one-out cross-validation, which was crucial in assessing model performance and ensuring that the Bayesian models generalize well.

- **modelsummary** [@modelsummary]: Summarizes statistical models in a clear, tabular format, supporting the communication of regression and other model results.

- **caret** [@caret]: Provides a unified interface for training and tuning machine learning models, offering numerous functions for preprocessing data, feature selection, model training, and performance evaluation, which streamline the process of creating predictive models.


- **_Telling Stories with Data_** [@tellingstories]: This book was referenced for its code and methodologies in presenting data and statistical information.



```{r}
#| warning: false
#| message: false
#| echo: false
#| label: tbl-survey_preview
#| tbl-cap: "Survey interview disruptions and factors influencing respondent comprehension"

# Load in data
analysis_data <- read_parquet(here::here("data/02-analysis_data/analysis_data.parquet"))


# Random sampling if needed
set.seed(131)
sampled_data <- analysis_data %>%
  sample_n(5000)

# Select important variables from the model
important_columns <- c(
  "respondent_understood_binary", "partner_interference", 
  "parent_interference",  
  "interviewer_age",  "country")

# Filter data to only include important columns
analysis_data_selected <- sampled_data %>%
  select(all_of(important_columns))

# Display the selected data with better column alignment
analysis_data_selected |>
  head(10) |>  # Limit to 10 rows to ensure readability
  kable(booktabs = TRUE, align = "c", col.names = c(
     "respondent_understood_binary", "partner_interference", 
  "parent_interference",  
  "interviewer_age",  "country"
  )) %>%
  kable_styling(full_width = FALSE)


```
@tbl-survey_preview presents the first six rows from the cleaned dataset, focusing on interview disruptions and respondent comprehension during social survey interviews.
## Outcome variables

Our analysis focuses on the following variables, with a specific focus on **respondent_understood_binary** as the dependent variable:

-   **respondent_understood_binary**: A binary variable indicating whether the respondent fully understood the survey questions. It was recoded as:

    -   *Understood*: Respondents who provided clear and accurate answers, indicating full comprehension of the questions.
    -   *Not understood*: Respondents who exhibited difficulties in understanding the questions, resulting in incomplete or unclear responses.

-   **interview_conduct_method**: The method by which the interview was conducted, with the following possible categories:

    -   *Face-to-face*: In-person interviews conducted by the surveyor.
    -   *Telephone*: Interviews conducted remotely via phone.
    -   *Self-administered*: Surveys completed by respondents themselves without direct interaction with a surveyor.

-   **child_interference**: Indicates whether interruptions by children occurred during the interview, with binary values:

    -   *Yes*: The interview was interrupted by children.
    -   *No*: No interruptions by children were recorded.

-   **partner_interference**: Indicates whether interruptions by partners occurred during the interview, with binary values:

    -   *Yes*: The interview was interrupted by the respondent's partner.
    -   *No*: No interruptions by the partner were recorded.

-   **question_clarification**: Tracks whether the respondent requested clarification on any survey questions. The variable includes:

    -   *Yes*: The respondent asked for clarification.
    -   *No*: The respondent did not ask for clarification.

-   **respondent_reluctant**: Captures the level of reluctance exhibited by the respondent during the interview process. Possible levels include:

    -   *Not reluctant*: The respondent showed no signs of hesitation.
    -   *Slightly reluctant*: The respondent exhibited mild hesitation.
    -   *Highly reluctant*: The respondent was clearly unwilling to engage with the survey.

-   **interviewer_age**: The age of the interviewer, used to explore whether interviewer demographics impact response quality.

-   **interviewer_gender**: The gender of the interviewer, categorized as:

    -   *Male*: Interviews conducted by male interviewers.
    -   *Female*: Interviews conducted by female interviewers.

These variables were chosen for their potential influence on response quality and survey reliability, allowing for a comprehensive analysis of disruptions and other contextual factors.

Detailed insights into the distribution of interviewer age across different countries are visualized in Figure 1, which presents the density plot of interviewer age for the top five countries in the dataset.

```{r}
#| label: fig-interviewer_age_density
#| fig-cap: "Density plot of Interviewer Age by Top 5 Countries"
#| echo: false
#| fig.width: 8
#| fig.height: 6
top_countries <- analysis_data %>%
  count(country, sort = TRUE) %>%
  top_n(5, n) %>%
  pull(country)

filtered_data <- analysis_data %>%
  filter(country %in% top_countries)

ggplot(filtered_data, aes(x = interviewer_age, fill = country, color = country)) +
  geom_density(alpha = 0.4) +
  labs(title = "Density Plot of Interviewer Age by Top 5 Countries", x = "Interviewer Age", y = "Density") +
  theme_minimal()

```

@fig-interviewer_age_density illustrates the distribution of interviewer ages across the top five countries in the dataset. The density plot highlights the variation in age demographics among interviewers from different countries, including Austria (AT), Germany (DE), Finland (FI), Great Britain (GB), and Ireland (IE).

From the plot, we can observe that certain countries, such as Germany and Austria, have a higher concentration of interviewers in specific age ranges, while others, like Ireland, show a more dispersed distribution. This visualization provides an overview of the age composition of interviewers across these countries, offering useful context for understanding potential differences in survey implementation.

```{r}
#| label: fig-response_understanding_effort_level
#| fig-cap: "Bar Plot of Response Understanding by Respondent Effort Level"
#| echo: false
#| fig.width: 8
#| fig.height: 6

ggplot(analysis_data, aes(x = respondent_tried_best, fill = respondent_understood_binary)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("#619CFF", "#F8766D")) +
  labs(
    title = "Bar Plot of Response Understanding by Respondent Tried Best",
    x = "Respondent Tried Best",
    y = "Proportion",
    fill = "Response Understanding"
  ) +
  theme_minimal()



```

@fig-response_understanding_effort_level Figure 2 illustrates the relationship between respondents' effort levels, as represented by "Respondent Tried Best," and their understanding of survey questions. The x-axis indicates different levels of effort, ranging from 1 (low effort) to 5 (high effort), along with an additional "Other" category. The y-axis shows the proportion of respondents who either understood or did not understand the survey questions.

From the bar plot, it is apparent that understanding levels vary slightly across different effort categories. In general, as the effort level increases, the proportion of respondents who understood the questions tends to be higher. Notably, effort levels 1, 2, and 3 exhibit a relatively higher portion of respondents who did not understand compared to effort levels 4 and 5, which show a more balanced distribution. The "Other" category presents a distinct pattern, differing slightly from the other effort levels. This visualization provides insight into how the level of effort during interviews corresponds to the respondents' comprehension of the questions asked.


```{r}
#| label: fig-interference_frequency
#| fig-cap: "Bar Plot of Interference Frequency by Respondent Tried Best"
#| echo: false
#| warning: false
#| fig.width: 8
#| fig.height: 6
#| 

ggplot(analysis_data, aes(x = respondent_tried_best, fill = as.factor(interference_present))) +
  geom_bar(position = "fill") +
  labs(
    title = "Bar Plot of Interference Frequency by Respondent Tried Best",
    x = "Respondent Tried Best",
    y = "Proportion",
    fill = "Interference Present"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Paired")





```

@fig-interference_frequency illustrates the frequency of interference during interviews across different levels of respondents' effort, as measured by the variable "Respondent Tried Best."

The bar plot shows the proportion of interviews with and without interference for each effort level, ranging from 1 (low effort) to 5 (high effort), along with an additional "Other" category.

From the plot, we can observe that interference is present across all effort levels, though its proportion varies slightly. Higher levels of effort (e.g., 4 and 5) tend to have a similar distribution of interference presence, while the "Other" category shows a slightly lower proportion of interference.This visualization provides an overview of how respondent effort and the presence of interference interact in the dataset.

## Justification

The variables selected for this study were chosen carefully based on their importance in understanding the quality and reliability of survey responses. These variables are derived from the European Social Survey dataset and focus on key aspects of the interview process:

-   **Respondent Understanding (RESPONDENT_UNDERSTOOD_BINARY)**: Indicates whether the respondent fully comprehended the survey questions, categorized as *Understood* or *Not Understood*.
-   **Interview Conduct Method (INTERVIEW_CONDUCT_METHOD)**: Refers to the method of conducting the interview, including face-to-face interviews, telephone interviews, and self-administered surveys.
-   **Interference Factors**:
    -   **Child Interference (CHILD_INTERFERENCE)**: Documents whether interruptions occurred due to children during the interview (*Yes* or *No*).
    -   **Partner Interference (PARTNER_INTERFERENCE)**: Indicates whether the respondent’s partner interrupted the interview (*Yes* or *No*).
-   **Question Clarification (QUESTION_CLARIFICATION)**: Records whether the respondent asked for clarification on survey questions at any point.
-   **Respondent Reluctance (RESPONDENT_RELUCTANT)**: Reflects the respondent’s level of reluctance during the interview, categorized into levels from *Not Reluctant* to *Highly Reluctant*.
-   **Interviewer Age (INTERVIEWER_AGE)**: Specifies the age of the interviewer, which is included to examine possible influences of interviewer demographics.
-   **Interviewer Gender (INTERVIEWER_GENDER)**: Indicates the gender of the interviewer, recorded as *Male* or *Female*.

Certain variables, such as **COUNTRY**, **SOCIOECONOMIC STATUS**, and **HOUSEHOLD COMPOSITION**, were excluded due to high levels of missing data or lesser relevance to the primary focus of the study. Similarly, variables with extensive categories were simplified to binary or grouped formats to enhance clarity and focus on essential distinctions.

After filtering and processing, the final dataset consists of **9,743 observations** and **25 variables**, structured to support a clear and concise examination of the study’s research objectives.

# Model {#sec-model}

### Model Set-up

Let $y_i$be the ordered categorical variable representing whether the respondent fully understood the survey questions for the $i$ -th interview. The predictors in the model include:

-   $\beta_1$: The coefficient for **interview_conduct_method**, representing the type of interview conducted. This variable includes different categories such as:

    -   Method 2
    -   Method 3
    -   Method 9
    -   (other possible interview methods)

-   $\beta_2$: The coefficient for **partner_interference**, representing different levels of interference by the respondent's partner. Categories include:

    -   Partner Interference Level 1
    -   Partner Interference Level 2
    -   (other possible levels of interference)

-   $\beta_3$: The coefficient for **child_interference**, indicating different levels of interference by children during the interview. This variable includes:

    -   Child Interference Level 1
    -   Child Interference Level 2
    -   (other possible levels of interference)

-   $\beta_4$: The coefficient for **parent_interference**, indicating different levels of interference by the respondent's parents.

-   $\beta_5$: The coefficient for **relative_interference**, representing interference by other relatives.

-   $\beta_6$: The coefficient for **non_relative_interference**, indicating whether interference was caused by non-relatives.

-   $\beta_7$: The coefficient for **interviewer_age**, which is a continuous variable representing the age of the interviewer.

-   $\beta_8$: The coefficient for **interviewer_gender**, indicating the gender of the interviewer:

    -   Male
    -   Female

-   $\beta_9$: The coefficient for **country** (as a factor), representing the country where the interview took place. The variable includes multiple countries.

-   $\beta_{10}$: The coefficient for **question_clarification**, indicating whether the respondent requested clarification during the interview. It includes multiple levels, such as:

    -   Clarification Level 2
    -   Clarification Level 3
    -   Clarification Level 4
    -   Clarification Level 5
    -   (other possible levels of clarification)

-   $\beta_{11}$: The coefficient for **respondent_reluctant**, representing the level of reluctance shown by the respondent. Categories include:

    -   Reluctant Level 2
    -   Reluctant Level 3
    -   Reluctant Level 4
    -   Reluctant Level 5
    -   (other possible levels of reluctance)

-   $\beta_{12}$: The coefficient for **respondent_tried_best**, indicating whether the respondent tried their best to provide accurate answers during the survey. Categories include:

    -   Tried Best Level 2
    -   Tried Best Level 3
    -   Tried Best Level 4
    -   Tried Best Level 5
    -   (other possible levels of effort)

Each coefficient $\beta_j$ represents the effect of the $j$-th predictor on the log-odds of the respondent understanding the survey questions.

-   $\eta_i$: The linear predictor or log-odds for the $i$-th observation, calculated by combining the intercept and the coefficients for each predictor variable.

-   $\kappa$: The set of thresholds (or cutpoints) that define the boundaries between the ordered categories of the outcome variable. In the ordered logistic regression model, these thresholds determine the ranges of $\eta_i$ values corresponding to each category of respondent understanding. The model estimates separate $\kappa$ values for each boundary between the ordered categories.

The model is specified as follows for ordered logistic regression:\begin{align}
y_i &\sim \text{OrderedLogistic}(\eta_i, \kappa) \\
\eta_i &= \beta_0 + \beta_{\text{interview\_conduct\_method}} \times \text{interview\_conduct\_method}_i \notag \\
       &\quad + \beta_{\text{partner\_interference}} \times \text{partner\_interference}_i \notag \\
       &\quad + \beta_{\text{child\_interference}} \times \text{child\_interference}_i 
       + \beta_{\text{parent\_interference}} \times \text{parent\_interference}_i \notag \\
       &\quad + \beta_{\text{non\_relative\_interference}} \times \text{non\_relative\_interference}_i 
       + \beta_{\text{interviewer\_age}} \times \text{interviewer\_age}_i \notag \\
       &\quad + \beta_{\text{interviewer\_gender}} \times \text{interviewer\_gender}_i 
       + \beta_{\text{country}} \times \text{country}_i \notag \\
       &\quad + \beta_{\text{question\_clarification}} \times \text{question\_clarification}_i 
       + \beta_{\text{respondent\_reluctant}} \times \text{respondent\_reluctant}_i \notag \\
       &\quad + \beta_{\text{respondent\_tried\_best}} \times \text{respondent\_tried\_best}_i \\
\beta &\sim \text{Normal}(0, 10) \quad (\text{default non-informative prior}) \\
\kappa &\sim \text{Normal}(0, 5) \quad (\text{default prior for cutpoints})
\end{align}

Where:

-   $\beta_0$ is the intercept of the model.
-   $\beta \sim \text{Normal}(0, 10)$: Represents a non-informative prior for the regression coefficients.
-   $\kappa \sim \text{Normal}(0, 5)$: Represents the prior for thresholds.

This model is designed to examine how different types of interference, interviewer characteristics, and respondent behaviors impact whether the respondent understood the survey questions, considering the ordered nature of the response.

## Prior distributions

In the Bayesian logistic regression model implemented using the `rstanarm` package, default prior distributions are applied to the model parameters to ensure robustness and reliability in inference. These priors are designed to balance the need for appropriate regularization with the flexibility to let the data guide the modeling:

-   **Intercept Priors**: For the intercept of the model, we use a normal prior distribution with a mean of 0 and a relatively large standard deviation, reflecting weakly informative priors. This choice helps stabilize the intercept term without imposing a strong prior belief, allowing the model to be flexible across diverse datasets.

-   **Coefficient Priors**: Coefficients in the model are assigned normal prior distributions, also with a mean of 0 but with a smaller standard deviation, typically set around 2.5. This helps prevent the individual predictors from exerting overly large influences unless strongly supported by the data. For example, variables such as different types of interference (e.g., partner, child, parent) or interview methods benefit from such priors as they maintain moderate regularization to avoid overfitting.

We ran the model in R [@citeR] using the `rstanarm` package@rstanarm. The default priors from `rstanarm` provide an appropriate level of smoothing and regularization, making the model applicable to survey data with complex patterns while still allowing the data to reveal meaningful relationships, especially concerning factors that affect response quality.

### Model justification

Existing research in survey methodology and social sciences suggests that factors such as interview method, the presence of disruptions during interviews, interviewer characteristics, and respondent behaviors significantly affect the quality of survey responses. In interviews with fewer disruptions, respondents are more likely to provide accurate and reliable answers. However, disruptions from various sources, such as partners, children, relatives, or even non-relatives, can compromise response quality. Additionally, interviewer characteristics—such as age and gender—may influence how comfortable respondents feel, which in turn affects the quality of their responses. The method of conducting the interview (e.g., face-to-face, phone, or self-administered) also plays a role in determining the overall effectiveness of data collection.

A Bayesian hierarchical logistic regression model was chosen for this analysis because the dependent variable, respondent_understood_binary, is binary in nature, indicating whether the respondent fully understood the survey questions. Logistic regression is a suitable approach when dealing with binary outcomes, and the Bayesian aspect helps in incorporating prior information and managing uncertainty in model parameters. This approach allows us to estimate the effect of various predictors—such as interview method, interference levels, and respondent reluctance—on the likelihood of understanding survey questions, providing a clear interpretation of how each factor contributes to the response quality.

The use of Bayesian hierarchical logistic regression is further justified because it allows the model to adapt flexibly to the data structure and manage potential variability across different interview scenarios. For example, the presence of multiple types of disruptions—each with different possible levels—requires a model that can adequately account for such hierarchical data structures. Additionally, Bayesian methods provide a framework for incorporating uncertainty and variability in interviewer and respondent characteristics, thereby giving a more nuanced and informed estimation. This model choice aligns well with established practices in survey research, where complex interaction effects and varying levels of measurement need to be addressed thoughtfully to ensure valid inferences about the factors influencing survey data quality.

# Results {#sec-result}

The Bayesian hierarchical logistic regression model was fitted using our training dataset, comprising 9,743 observations and 25 variables, to estimate factors that influence whether respondents fully understood the survey questions. @tbl-summary-model presents the summary of the model's key coefficients, including posterior mean estimates and credible intervals for each parameter, providing insights into the impact of interview disruptions, interviewer characteristics, and survey methods on response quality.

### Key Findings from the Coefficients

The analysis reveals that different types of disruptions and characteristics of the interview environment have a distinct impact on respondents’ comprehension of survey questions. Here are some of the key findings:

-   **Interview Method**: Interview Method 9 was found to have an exceptionally large effect (Mean = 95.1), indicating it significantly improves the likelihood that respondents understand the questions. Conversely, other interview methods such as **Method 2** and **Method 3** had moderate effects, suggesting that the effectiveness of an interview style can vary substantially based on how it is implemented.

-   **Disruptions During the Interview**:

    -   **Partner Interference** had a negative effect on the quality of responses (Mean = -0.6). This indicates that the presence of a partner during the interview tends to lower the ability of the respondent to fully understand and accurately respond.
    -   **Child Interference** and **Parent Interference** also showed notable negative impacts (Means = -0.8 and -1.6 respectively), highlighting that family-related disruptions can significantly hinder the respondent’s ability to provide reliable answers.
    -   Interestingly, **Non-relative Interference** was associated with a positive effect (Mean = 1.0), suggesting that such disruptions may not be as distracting or might even motivate respondents to be more attentive.

-   **Country Effects**: The model also included a factor for **country** to account for geographic variability in survey conditions. The results show variability in respondent comprehension across different countries. For instance:

    -   **Germany (DE)** and **Hungary (HU)** had negative coefficients, indicating lower response comprehension in these countries.
    -   In contrast, **Finland (FI)** showed neutral effects, reflecting possibly consistent survey conditions in that country.

-   **Question Clarification**: Levels of **question clarification** were consistently found to negatively affect response quality. For example, **Clarification Level 4** had a mean of -3.0, implying that respondents requiring repeated clarification were less likely to fully understand the questions.

-   **Respondent Reluctance**: Increased **respondent reluctance** also had a detrimental effect on comprehension. For example, **Reluctant Level 3** had a mean coefficient of -0.6, indicating hesitation or reluctance results in poorer quality responses.

### Summary and Interpretation

As detailed in @tbl-summary-model, the summary of coefficients provides a quantitative perspective on how each factor contributes to whether respondents fully understand survey questions. Notably:

-   **Parental Interference** displayed the most substantial negative impact (Mean = -1.6), indicating that the presence of parents during an interview session significantly reduces comprehension levels. This suggests that minimizing parental presence could enhance the quality of responses.
-   Conversely, **Non-relative Interference** was associated with a positive effect on understanding (Mean = 1.0), implying that some types of outside influence might actually promote a more careful engagement with the questions, perhaps because respondents feel more observed or responsible for their responses.

```{r}
#| label: tbl-summary-model
#| tbl-cap: "Summary of key coefficients from the Bayesian hierarchical logistic regression model analyzing survey response quality"
#| echo: false
#| warning: false
library(rstanarm)

first_model <-
  readRDS(file = here::here("models/model.rds"))
summary <- summary(first_model)

summary_table <- head(summary, 10)[, 1:1]

kable(col.names = c(" ", "coefficient"),
  summary_table, digits = 3)
```

```{r}
#| label: fig-coefficient-intervals
#| fig-cap: "The 90% credible intervals for all model coefficients from the Bayesian hierarchical logistic regression"
#| echo: false
#| warning: false
#| fig.width: 8
#| fig.height: 6


model_summary <- tidy(first_model, conf.int = TRUE, conf.level = 0.90)



important_parameters <- c(
  "interview_conduct_method2", 
  "partner_interference1", 
  "child_interference1",
  "parent_interference1", 
  "non_relative_interference1", 
  "interviewer_gender2", 
  "interviewer_age"
)

filtered_summary <- model_summary[model_summary$term %in% important_parameters, ]




ggplot(filtered_summary, aes(x = estimate, y = term)) +
  geom_point(size = 3, color = "blue") +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.3, color = "blue") +
  labs(
    title = "Model Coefficients",
    subtitle = "90% credible intervals",
    x = "Coefficient Estimate",
    y = "Parameters"
  ) +
  theme_minimal() +
  xlim(-4, 4) 
```

The Bayesian hierarchical logistic regression model's estimates are visualized in @fig-coefficient-intervals. Each point on the plot represents the estimated mean effect size of the predictor variables on the likelihood that the respondent understood the questions well, while the lines indicate the 90% credible intervals. These estimates reveal several key insights regarding factors that affect response quality in survey interviews:

The variable partner_interference1 has a significant negative effect, indicating that when a partner is present during the interview, the quality of the respondent's understanding tends to decrease. This might be due to partners interrupting or distracting the respondent, which lowers their focus.

The variable non_relative_interference1 shows the highest positive effect among the predictors, suggesting that interference from non-family members can sometimes enhance the respondent's performance. This could be due to increased formality or caution when responding in the presence of outsiders, leading to more careful answers.

The variable interviewer_gender2 exhibits a slight negative effect, implying that female interviewers may encounter lower respondent understanding under certain conditions. However, the wide credible interval suggests that more data is needed to definitively determine the significance of this effect.

interview_conduct_method2 also displays a negative effect, indicating that certain interview methods (such as telephone interviews) might not facilitate respondent comprehension as effectively as face-to-face methods.

interviewer_age has an estimated effect close to zero with a broad credible interval, suggesting that interviewer age does not have a substantial influence on respondent understanding.

These findings highlight the importance of considering the type of interference during interviews and specific characteristics of interviewers. It is evident that some types of disturbances and interview formats could significantly affect the reliability of collected data. By mitigating negative factors such as partner interference and selecting appropriate interview methods, the quality of survey data can be substantially improved, leading to more reliable conclusions in social survey research.

# Discussion {#sec-discussion}

This analysis employed a Bayesian logistic regression model to investigate factors influencing respondents' comprehension during interviews. By incorporating various types of interference, interview methods, and respondent characteristics, the model elucidates how these variables impact understanding during social survey interviews.

## Factors Affecting Response Comprehension

The model analysis reveals that different types of interference significantly affect respondents' comprehension during interviews. Notably, partner and child interference have a negative impact on understanding. For example, the coefficient for partner interference is significantly negative (mean = -0.6), suggesting that the presence of a partner may hinder a respondent's full comprehension of the questions. This finding aligns with previous research, indicating that social pressure or disturbances from nearby individuals can negatively impact respondents' attention during surveys.

In contrast, interviewer characteristics, such as gender, show little effect on comprehension. This suggests that while environmental and relational interference plays a crucial role, interviewer attributes do not significantly influence respondents' comprehension in the context of this study.

## Implications for Managing Interview Interference

The strategic implication of these findings is that managing the interview environment is crucial for improving data quality. Since partner and child interference are associated with a reduced likelihood of comprehension, measures should be considered to minimize such disruptions. This might include conducting interviews in a more private setting or explicitly instructing respondents to minimize interruptions during the interview.

Variables related to the interview method also show significant impact. The coefficient for “interview_conduct_method3” is positive, indicating that certain specific methods, possibly involving higher levels of interaction or explanation, contribute to improved comprehension. This implies that training interviewers to adopt methods that emphasize clarity and engagement could enhance respondents' understanding, even in the presence of interference.

## Limitations and Areas for Improvement

A key limitation of the model is the generalizability of its findings, particularly regarding different population groups. The current dataset may not capture all cultural or socioeconomic factors that could influence comprehension. For instance, while country-level effects are included, many of these effects are either negative or negligible, suggesting that unmeasured factors may explain differences between regions.

Additionally, the model's predictive performance, assessed through the Bayesian R-squared value, is moderate (mean R² around 0.32). This indicates that while the model accounts for some variability in response comprehension, a substantial amount remains unexplained. Future research could incorporate additional variables, such as respondents' education levels or prior survey experience, to enhance model fit and explanatory power.

## Future Research Directions

Building on these findings, future research could explore more granular aspects of interference, such as the intensity or duration of disruptions. Understanding whether brief interruptions have a different effect compared to prolonged interference could help develop more precise guidelines for survey management.

Further research could also examine interaction effects between variables, such as how partner interference affects respondents with varying levels of willingness to participate. This could unveil more targeted strategies to improve survey conditions, ultimately leading to higher data reliability.

## Practical Insights for Survey Management

The results of this analysis underscore the importance of optimizing survey conditions to enhance respondent comprehension. Minimizing interference and adopting effective interview methods can significantly improve the quality of collected data. Survey administrators might consider providing pre-interview guidance to help respondents reduce disruptions and training interviewers in techniques proven to enhance comprehension.

The significance of certain variables, such as partner interference, also highlights the complexity of social dynamics during surveys. Addressing these dynamics is crucial for ensuring the accuracy of survey results, which directly impacts the validity of conclusions drawn from the data.

Overall, the study provides actionable insights into how survey administrators can better structure interview conditions to mitigate interference and enhance respondent understanding, thereby improving the reliability and validity of survey data.



\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}
## Model summary
```{r}
#| label: tbl-summary
#| tbl-cap: "Coefficients from a Bayesian logistic regression model examining factors affecting respondents' understanding during interviews, including interview methods, interference types, and country-level indicators."
#| echo: false
#| warning: false
modelsummary(
  list("Bayesian Logistic Regression" = first_model),
  metrics = c("R2", "RMSE")
)
```
@tbl-summary presents the coefficients from our model analyzing factors that influence response comprehension during interviews. The intercept value, 1.823, represents the baseline level of response comprehension when all other predictors are at their reference or baseline levels. Key variables include "interview_conduct_method9," which has a highly positive coefficient of 81.309, indicating that this particular interview method greatly enhances respondent comprehension. In contrast, factors such as "parent_interference" and "child_interference" exhibit negative coefficients (-1.586 and -0.623 respectively), suggesting that these types of interference are associated with lower levels of understanding during interviews.

Additionally, non-relative interference, which shows a positive coefficient of 0.941, suggests that having individuals outside the immediate family present may be less disruptive, or even beneficial, for the comprehension of survey questions. Interviewer-related variables, such as "interviewer_gender," appear to have minimal effects on comprehension, as evidenced by small coefficients like 0.100, suggesting that interviewer attributes play a relatively minor role compared to environmental and relational factors during social surveys
## VIF
```{r}
#| label: tbl-vif
#| tbl-cap: "Variance Inflation Factors (VIF) for Each Predictor in the Model."
#| echo: false
#| warning: false

vif_values <- vif(first_model)

gvif_df <- data.frame(
  Variable = rownames(vif_values), 
  GVIF = round(vif_values[, "GVIF"], 3),
  Df = vif_values[, "Df"],
  `GVIF^(1/(2*Df))` = round(vif_values[, "GVIF^(1/(2*Df))"], 3)
)

kable(gvif_df)

```
@tbl-vif presents the Generalized Variance Inflation Factors (GVIF) for each predictor in the Bayesian logistic regression model. GVIF helps assess multicollinearity among predictors, indicating how much the variance of a regression coefficient is inflated due to correlations between variables.

Most predictors, such as respondent_tried_best (GVIF = 3.186, Df = 5) and respondent_reluctant (GVIF = 2.725, Df = 5), show moderate GVIF values, suggesting manageable multicollinearity. The adjusted GVIF values, which are generally close to 1, indicate that the inflation effect is controlled across variables.

In summary, the GVIF analysis indicates that multicollinearity is present but not severe enough to threaten model reliability. Further analysis may focus on predictors with slightly higher GVIF to ensure stability.

## Posterior predictive check
```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheck
#| fig-cap: "Posterior Predictive Check: The comparison between observed response values (dark line) and model-generated replications (light lines) demonstrates the model's ability to replicate the observed response patterns, indicating a good fit and capturing the main features of the response distribution."


pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")+ xlim(-0.1, 1.2)

```
In @fig-ppcheck, we implement a posterior predictive check, which displays the overlap between the observed data (denoted by \( y \)) and the replicated data generated from the model (denoted by \( y_{\text{rep}} \)). The replicated lines closely follow the shape and central trend of the observed data, particularly around the main peak at 0.9, which suggests that the model captures the overall distribution effectively. This alignment indicates that the model successfully represents the underlying data-generating process, especially in regions with high data density. While the model's performance appears strong, any visible discrepancies—particularly in areas of lower density—may highlight aspects of the data structure that are not fully captured. Addressing such discrepancies could involve refining the model by introducing additional parameters or reconsidering underlying assumptions. Overall, this comparison provides evidence that the model fits well, though there remain potential areas for enhancement to better encompass the full range of data complexities.


## Prediction
```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: tbl-comprehension-matrix
#| tbl-cap: "Confusion Matrix for Comprehension Classification"
#| tbl-subcap: ["Counts of Predictions vs Reference", "Performance Metrics Summary"]
#| layout-ncol: 2
analysis_data_train <- read_parquet(here::here("data/02-analysis_data/train_data.parquet"))
analysis_data_test <- read_parquet(here::here("data/02-analysis_data/test_data.parquet"))
factor_vars <- c("interview_conduct_method", "partner_interference", "child_interference",
                 "parent_interference", "relative_interference", "non_relative_interference",
                 "country", "interviewer_gender", "question_clarification", "respondent_reluctant",
                 "respondent_tried_best")

# Iterate through factor variables and set the factor levels in test data to be consistent with train data
for (var in factor_vars) {
  if (is.factor(analysis_data_train[[var]])) {
    # Set factor levels in test data to match those in train data
    analysis_data_test[[var]] <- factor(analysis_data_test[[var]], levels = levels(analysis_data_train[[var]]))
  }
}

# Predict probabilities and convert predicted_class to "Understood" and "Not_understood"
predicted_prob <- predict(first_model, newdata = analysis_data_test, type = "response")
analysis_data_test$predicted_class <- ifelse(predicted_prob > 0.5, "Understood", "Not_understood")

# Convert predicted_class and respondent_understood_binary to factors with consistent levels
analysis_data_test$predicted_class <- factor(analysis_data_test$predicted_class, levels = c("Not_understood", "Understood"))
analysis_data_test$respondent_understood_binary <- factor(analysis_data_test$respondent_understood_binary, levels = c("Not_understood", "Understood"))

# Display table of prediction results
table(analysis_data_test$predicted_class)
table(analysis_data_test$respondent_understood_binary)


confusion_matrix <- confusionMatrix(
  analysis_data_test$predicted_class,
  analysis_data_test$respondent_understood_binary
)


confusion_matrix <- matrix(c(41, 36, 129, 3970), nrow = 2, byrow = TRUE,
                           dimnames = list("Prediction" = c("Not_understood", "Understood"),
                                           "Reference" = c("Not_understood", "Understood")))

# Extract True Positive, False Positive, True Negative, and False Negative from the confusion matrix
TP <- confusion_matrix["Not_understood", "Not_understood"]
FP <- confusion_matrix["Not_understood", "Understood"]
TN <- confusion_matrix["Understood", "Understood"]
FN <- confusion_matrix["Understood", "Not_understood"]

# Calculate evaluation metrics
accuracy <- (TP + TN) / sum(confusion_matrix)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
specificity <- TN / (TN + FP)
fpr <- FP / (FP + TN) # False Positive Rate
fnr <- FN / (FN + TP) # False Negative Rate
f1_score <- 2 * (precision * recall) / (precision + recall)
balanced_accuracy <- (recall + specificity) / 2

# Create a data frame for metrics
metrics_df <- data.frame(
  Metric_Name = c("Accuracy", 
                  "Precision (Positive Predictive Value, PPV)",
                  "Recall (True Positive Rate, TPR)", 
                  "True Negative Rate (TNR, Specificity)", 
                  "False Positive Rate (FPR)", 
                  "False Negative Rate (FNR)", 
                  "F1 Score", 
                  "Balanced Accuracy"),
  Value = round(c(accuracy, precision, recall, specificity, fpr, fnr, f1_score, balanced_accuracy), 4)
)

# Print the metrics table
kable(metrics_df, caption = "Evaluation Metrics for Classification Model")




confusion_df <- as.data.frame(as.table(confusion_matrix))

confusion_df %>%
  kable("html", col.names = c("Reference", "Not_understood", "Understood"), 
        caption = "Confusion Matrix for Comprehension Classification") %>%
  kable_styling(full_width = FALSE)



```
@tbl-comprehension-matrix-2 presents the confusion matrix for the comprehension classification model. The matrix provides the counts of correct and incorrect classifications between the "Not_understood" and "Understood" categories, highlighting how well the model performed in predicting respondent comprehension. The diagonal elements represent the correctly classified observations, while the off-diagonal elements indicate misclassifications. The matrix helps visualize the strengths and weaknesses of the model, particularly in distinguishing between respondents who understood versus those who did not.

@tbl-comprehension-matrix-1 shows the evaluation metrics for the classification model. These metrics include Accuracy, Precision (Positive Predictive Value), Recall (True Positive Rate), True Negative Rate (Specificity), and F1 Score, among others. The metrics provide a comprehensive summary of the model's performance, measuring not only how many predictions were correct, but also evaluating the balance between sensitivity and specificity. The high specificity and overall accuracy indicate that the model is effective in identifying respondents who understood the questions, but the low recall suggests limitations in detecting those who did not understand.
## Diagnostics
### Validation
```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-diagnostic-checks
#| fig-cap: "Comparing Diagnostic Visualizations for Model Checking"
#| fig-subcap: ["Residual Histogram", "Pareto k Diagnostic Values"]
#| layout-ncol: 2

# Generate Residual Histogram
residuals <- residuals(first_model, type = "response")
hist(residuals, main = "Residual Histogram", xlab = "Residuals")

# Perform LOO cross-validation and assign to loo_results
loo_results <- loo(first_model)

# Extract Pareto k values from loo results and create the Pareto k Diagnostic plot
pareto_k <- loo_results$diagnostics$pareto_k
pareto_k_df <- data.frame(Index = 1:length(pareto_k), Pareto_k = pareto_k)

ggplot(pareto_k_df, aes(x = Index, y = Pareto_k)) +
  geom_bar(stat = "identity", fill = "orange") +
  geom_hline(yintercept = 0.7, linetype = "dashed", color = "red") +
  labs(title = "Pareto k Diagnostic Values", x = "Observation Index", y = "Pareto k Value") +
  theme_minimal()



```
In Figure @fig-diagnostic-checks-1, the residual histogram displays the distribution of residuals from the model. The concentrated frequency around zero suggests that the model effectively captures much of the variability in the data, with fewer extreme deviations, thereby indicating a reasonable level of fit.

Figure @fig-diagnostic-checks-2 presents the Pareto k diagnostic values, which help assess the reliability of the estimated leave-one-out cross-validation (LOO) predictions. The majority of Pareto k values fall below the threshold of 0.7, indicating that the model is well-calibrated with only minimal issues in terms of influential observations.
```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-loo-rhat-checks
#| fig-cap: "Comparing Diagnostic Visualizations for Model Checking"
#| fig-subcap: ["Leave-One-Out Cross Validation (ELPD_LOO)", "Rhat Diagnostic Plot"]
#| layout-ncol: 2
# Create Leave-One-Out Cross Validation (ELPD_LOO) plot
loo_df <- data.frame(Observation = 1:length(loo_results$pointwise[,"elpd_loo"]),
                     ELPD_LOO = loo_results$pointwise[,"elpd_loo"])

ggplot(loo_df, aes(x = Observation, y = ELPD_LOO)) +
  geom_point(color = "darkgreen") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Leave-One-Out Cross Validation (ELPD_LOO)", x = "Observation Index", y = "ELPD_LOO") +
  theme_classic()

# Generate Rhat Diagnostic Plot
rhat_values <- rhat(first_model)
mcmc_rhat(rhat_values)

```

Figure @fig-loo-rhat-checks-1 is a scatter plot for leave-one-out cross-validation (ELPD_LOO) estimates. The consistency of the points around the red loess smooth line suggests that the model’s predictions are largely consistent across different observations, thus reinforcing the model's robustness.

Figure @fig-loo-rhat-checks-2 is an Rhat plot, showing that all values of the Rhat diagnostic are close to 1. This indicates good mixing of the Markov Chain Monte Carlo (MCMC) chains and suggests that the model has likely converged appropriately, making the posterior estimates trustworthy.



\newpage



# References
