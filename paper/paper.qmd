---
title: "The Impact of Interview Disruptions on Data Quality in Social Surveys"
subtitle: "Analyzing How Various Interferences Affect Respondent Accuracy and Survey Reliability"
author: 
  - Dezhen Chen
thanks: "Code and data are available at: https://github.com/1Dezhenchen/The-Influence-of-Interview-Disruptions-on-Respondent-Data-Quality-in-Social-Surveys."
date: today
date-format: long
abstract: "In social surveys, disruptions during interviews can significantly impact the quality of respondent data, which in turn affects the reliability of the survey results. This study uses Bayesian hierarchical logistic regression to analyze how various factors—such as the type of disruption, the presence of different family members, interview conduct method, interviewer characteristics, and respondent attitudes—affect the accuracy and quality of responses. Our analysis shows that different types of disruptions have distinct impacts on the quality of responses, revealing critical insights into optimizing survey conditions to improve data reliability and validity."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(arrow)
library(knitr)
library(rstanarm)
library(tidyverse)
library(ggplot2)
library(bayesplot)
library(ggridges)
```


# Introduction
In recent years, the quality of social survey data has become an important topic of inquiry, especially with regard to how different disruptions impact respondent behavior. Several studies have contributed to this body of knowledge by focusing on factors that may affect the reliability of survey responses. Groves et al. (2009) examined the role of interviewer characteristics—such as experience and demeanor—in shaping the quality of responses through a multi-level modeling approach. In another key study, Couper and de Leeuw (2003) investigated how different modes of conducting interviews, like in-person versus telephone, influenced the comfort and attentiveness of respondents. Fowler and Mangione (1990) took a different approach by analyzing observational data to assess the impact of third-party presence during interviews. Although these works laid a valuable foundation for understanding data quality, there remains a gap in systematically analyzing the distinct effects of various kinds of disruptions, such as familial or environmental interference, on respondent understanding and response accuracy.

This study endeavors to address these overlooked aspects by closely examining the role of interview disruptions on data quality in a major social survey. Specifically, our analysis explores the influence of different types of interferences—including interruptions from partners, children, and other family members—on respondents' comprehension and engagement during the interview process. We employed a Bayesian logistic regression model utilizing the rstanarm package in R to capture these influences. The model incorporates a range of predictor variables, such as interviewer characteristics (age and gender), different methods of interview administration, and family-related interferences. During data preparation, the categorical levels of several predictor variables were simplified, excluding the 'country' variable, to ensure interpretability and to streamline model estimation. We also included indicators of respondent attitudes, such as whether questions were clarified during the interview or if reluctance to answer was observed, aiming for a more comprehensive understanding of respondent engagement.

Our results reveal compelling insights into the effects of these disruptions on data quality. We found that family-related interferences, notably from partners and parents, were associated with reduced comprehension on the part of respondents. The findings also suggested that interviews conducted in-person resulted in significantly higher levels of respondent understanding compared to those conducted by telephone. Additionally, older interviewers generally achieved better data quality, which could reflect an ability to manage disruptions more effectively or employ better rapport-building techniques. The influence of the interviewer's gender was also noted, though it appeared to interact in more complex ways with other factors such as the mode of interview and the specific type of interference. Taken together, these findings underscore the importance of managing both environmental and personal factors during interviews to improve data accuracy.

Beyond merely quantifying these effects, our study provides deeper implications for enhancing data collection practices in the field. The evidence points to the need for improved interviewer training focused on managing disruptions effectively, which can help ensure the collection of high-quality data even in less-than-ideal circumstances. The significant negative effects of family interference suggest that in-home surveys could benefit from targeted strategies to minimize such disruptions, including adjusting the timing of interviews or providing incentives for privacy during responses. Furthermore, the results highlight the importance of understanding respondent reluctance and tailoring interviewer interactions to reduce its occurrence. These insights have relevance for not only improving the current practices of social surveys but also for shaping future methodologies in an increasingly diverse and complex interviewing landscape.

The remainder of this paper is organized to present a comprehensive exploration of these topics. Section 2 provides a detailed overview of the dataset, focusing on the different variables and their role in assessing data quality. It also outlines the preprocessing steps taken, including data cleaning and the recoding of variables. Section 3 introduces the Bayesian logistic regression methodology, explaining the statistical approach and justifying the selection of priors for the analysis. In Section 4, we present the empirical findings, including parameter estimates and an interpretation of the relationships uncovered by the model. Section 5 discusses the practical implications of these findings, emphasizing the improvements needed in interviewer training and the strategies for reducing respondent interference. Finally, Section 6 summarizes the key contributions of this research, outlines the limitations, and suggests avenues for future work—particularly concerning the integration of new data collection technologies and adaptive survey designs aimed at mitigating the impact of disruptions on respondent data quality.




# Estimand
This study aims to evaluate the factors that influence the quality and reliability of survey responses during interview disruptions in social surveys. By analyzing variables such as different types of interferences (partner, child, parent, and relative interferences), interview methodologies, the demographic characteristics of interviewers (such as age and gender), the language of the questionnaire, and respondent behaviors like reluctance, clarification requests, and effort put into answering, this research seeks to understand how these elements affect the consistency and clarity of responses. The core goal is to determine the impact of these factors on respondent understanding and data reliability, thereby providing insights into improving survey methods and reducing biases in data collection.

# Data {#sec-data}

## Overview

We use the statistical programming language R (R Core Team 2023) to process, analyze, and visualize survey data exploring the impact of interview disruptions on response quality. The dataset used in this study includes several subsets tailored to different stages of the analysis, enabling a systematic and detailed examination of the research questions. The main dataset for modeling consists of 9,743 rows and 25 variables, capturing detailed information about different types of disruptions (e.g., partner, child, or relative interference), interview methods, and respondent characteristics.

To ensure the reliability of the analysis, we filtered the data to exclude incomplete or ambiguous responses for key variables, such as missing values in "respondent_understood." Responses were also recoded into binary categories to focus on meaningful distinctions in response quality, while variables with excessive categories were grouped to improve interpretability. These preprocessing steps were essential to refine the dataset and facilitate robust modeling.

The following R packages were employed to streamline the data analysis workflow:

- **tidyverse** (Wickham et al. 2021): Used for efficient data manipulation, cleaning, and transformation. It includes:
- **dplyr** (Wickham, François, et al. 2021): For summarizing and managing complex datasets.
- **ggplot2** (Wickham 2021): For creating powerful and flexible visualizations.
  
- **rstanarm** (Team 2021): Facilitated the implementation of Bayesian models, enabling straightforward regression modeling using Stan.

- **arrow** (Apache Arrow 2021): Employed for efficiently reading and writing large datasets, enhancing data handling capabilities across systems.

- **car** (Fox et al. 2021): Provided functions for regression diagnostics and other statistical analyses, particularly useful for checking assumptions and model validity.

- **brms** (Bürkner 2021): A package for Bayesian regression modeling, which was used to implement more complex modeling approaches for the data.

- **rsample** (Kuhn 2021): Used for creating resampling datasets, ensuring robust model validation by facilitating cross-validation and bootstrapping techniques.

- **janitor** (Firke 2021): Simplified data cleaning processes by helping manage variable names and data structures, making raw data easier to handle.

- **GGally** (Schloerke et al. 2021): Used to enhance ggplot2 visualizations by providing additional functions for correlation plots and pairwise visualizations, which are useful for exploratory data analysis.
This study aims to provide practical insights into how various disruptions affect data quality in survey settings, helping to optimize survey design and execution in the future.
## Measurement
	
The data for this study is sourced from the European Social Survey (ESS), which collects information on societal attitudes and behaviors across multiple European countries. The dataset focuses on respondents’ understanding of survey questions, their cooperation levels, and the impact of environmental factors such as interruptions during interviews. All responses are collected using standardized methods to ensure comparability across countries.

Different countries may implement localized adaptations in survey administration, which could influence response behaviors. To account for this, country-level metadata is included, allowing for the analysis of cross-national variations.
```{r}
#| warning: false
#| message: false
#| echo: false
#| label: tbl-aerial_preview
#| tbl-cap: "Survey responses on the impact of interview disruptions across countries in social surveys"

# Load in data
analysis_data <- read_parquet(here::here("data/02-analysis_data/analysis_data.parquet"))

# Random sampling if needed
set.seed(131)
sampled_data <-analysis_data %>%
  sample_n(5000)

analysis_data |>
  head() |>
  kable(booktabs = TRUE, align = "c")
```

## Outcome variables

Our analysis focuses on the following variables, with a specific focus on **respondent_understood_binary** as the dependent variable:

- **respondent_understood_binary**: A binary variable indicating whether the respondent fully understood the survey questions. It was recoded as:
  - *Understood*: Respondents who provided clear and accurate answers, indicating full comprehension of the questions.
  - *Not understood*: Respondents who exhibited difficulties in understanding the questions, resulting in incomplete or unclear responses.

- **interview_conduct_method**: The method by which the interview was conducted, with the following possible categories:
  - *Face-to-face*: In-person interviews conducted by the surveyor.
  - *Telephone*: Interviews conducted remotely via phone.
  - *Self-administered*: Surveys completed by respondents themselves without direct interaction with a surveyor.

- **child_interference**: Indicates whether interruptions by children occurred during the interview, with binary values:
  - *Yes*: The interview was interrupted by children.
  - *No*: No interruptions by children were recorded.

- **partner_interference**: Indicates whether interruptions by partners occurred during the interview, with binary values:
  - *Yes*: The interview was interrupted by the respondent's partner.
  - *No*: No interruptions by the partner were recorded.

- **question_clarification**: Tracks whether the respondent requested clarification on any survey questions. The variable includes:
  - *Yes*: The respondent asked for clarification.
  - *No*: The respondent did not ask for clarification.

- **respondent_reluctant**: Captures the level of reluctance exhibited by the respondent during the interview process. Possible levels include:
  - *Not reluctant*: The respondent showed no signs of hesitation.
  - *Slightly reluctant*: The respondent exhibited mild hesitation.
  - *Highly reluctant*: The respondent was clearly unwilling to engage with the survey.

- **interviewer_age**: The age of the interviewer, used to explore whether interviewer demographics impact response quality.

- **interviewer_gender**: The gender of the interviewer, categorized as:
  - *Male*: Interviews conducted by male interviewers.
  - *Female*: Interviews conducted by female interviewers.

These variables were chosen for their potential influence on response quality and survey reliability, allowing for a comprehensive analysis of disruptions and other contextual factors. 


Detailed insights into the distribution of interviewer age across different countries are visualized in Figure 1, which presents the density plot of interviewer age for the top five countries in the dataset.
```{r}
#| label: fig-interviewer_age_density
#| fig-cap: "Density plot of Interviewer Age by Top 5 Countries"
#| echo: false
#| fig.width: 8
#| fig.height: 6
top_countries <- analysis_data %>%
  count(country, sort = TRUE) %>%
  top_n(5, n) %>%
  pull(country)

filtered_data <- analysis_data %>%
  filter(country %in% top_countries)

ggplot(filtered_data, aes(x = interviewer_age, fill = country, color = country)) +
  geom_density(alpha = 0.4) +
  labs(title = "Density Plot of Interviewer Age by Top 5 Countries", x = "Interviewer Age", y = "Density") +
  theme_minimal()

```

@fig-interviewer_age_density illustrates the distribution of interviewer ages across the top five countries in the dataset. The density plot highlights the variation in age demographics among interviewers from different countries, including Austria (AT), Germany (DE), Finland (FI), Great Britain (GB), and Ireland (IE).

From the plot, we can observe that certain countries, such as Germany and Austria, have a higher concentration of interviewers in specific age ranges, while others, like Ireland, show a more dispersed distribution. This visualization provides an overview of the age composition of interviewers across these countries, offering useful context for understanding potential differences in survey implementation.
```{r}
#| label: fig-interference_frequency
#| fig-cap: "Bar Plot of Interference Frequency by Respondent Tried Best"
#| echo: false
#| fig.width: 8
#| fig.height: 6

ggplot(analysis_data, aes(x = respondent_tried_best, fill = as.factor(interference_present))) +
  geom_bar(position = "fill") +
  labs(
    title = "Bar Plot of Interference Frequency by Respondent Tried Best",
    x = "Respondent Tried Best",
    y = "Proportion",
    fill = "Interference Present"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Paired")



```
@fig-interference_frequency illustrates the frequency of interference during interviews across different levels of respondents’ effort, as measured by the variable "Respondent Tried Best." The bar plot shows the proportion of interviews with and without interference for each effort level, ranging from 1 (low effort) to 5 (high effort) and an additional "Other" category.

From the plot, we can observe that interference is present across all effort levels, though its proportion varies slightly. Higher levels of effort (e.g., 4 and 5) tend to have a similar distribution of interference presence, while the "Other" category shows a slightly lower proportion of interference. This visualization provides an overview of how respondent effort and the presence of interference interact in the dataset.

```{r}
#| label: fig-ridgeline_interviewer_age
#| fig-cap: "Ridgeline Plot of Interviewer Age by Question Clarification Level"
#| echo: false
#| fig.width: 8
#| fig.height: 6
#| 
ggplot(analysis_data, aes(x = interviewer_age, y = as.factor(question_clarification), fill = ..x..)) +
  geom_density_ridges_gradient() +
  scale_fill_viridis_c() +
  labs(
    title = "Ridgeline Plot of Interviewer Age by Question Clarification Level",
    x = "Interviewer Age",
    y = "Question Clarification Level"
  ) +
  theme_minimal()



```
- **@fig-ridgeline_interviewer_age** shows a ridgeline plot of interviewer age distributions across different levels of question clarification requests. 
- The x-axis represents interviewer age, while the y-axis categorizes responses by clarification levels (1 to 5) and an "Other" group.
- Each ridge represents the density distribution of interviewer age for a specific clarification level.
- Lighter colors indicate higher densities within the corresponding age range.

### Key Observations:
- Interviewers aged 40 to 50 are prominent in levels 4 and 5, showing higher densities in these groups.
- Clarification levels 1 and 2 show a broader distribution of interviewer ages, with no distinct peak.
- The "Other" category exhibits a more balanced spread of ages, without significant density concentrations.

This visualization highlights the distribution of interviewer age across clarification levels and provides insights into age-related patterns in survey interactions.


##  Justification

The variables selected for this study were chosen carefully based on their importance in understanding the quality and reliability of survey responses. These variables are derived from the European Social Survey dataset and focus on key aspects of the interview process:

- **Respondent Understanding (RESPONDENT_UNDERSTOOD_BINARY)**: Indicates whether the respondent fully comprehended the survey questions, categorized as *Understood* or *Not Understood*.
- **Interview Conduct Method (INTERVIEW_CONDUCT_METHOD)**: Refers to the method of conducting the interview, including face-to-face interviews, telephone interviews, and self-administered surveys.
- **Interference Factors**:
  - **Child Interference (CHILD_INTERFERENCE)**: Documents whether interruptions occurred due to children during the interview (*Yes* or *No*).
  - **Partner Interference (PARTNER_INTERFERENCE)**: Indicates whether the respondent’s partner interrupted the interview (*Yes* or *No*).
- **Question Clarification (QUESTION_CLARIFICATION)**: Records whether the respondent asked for clarification on survey questions at any point.
- **Respondent Reluctance (RESPONDENT_RELUCTANT)**: Reflects the respondent’s level of reluctance during the interview, categorized into levels from *Not Reluctant* to *Highly Reluctant*.
- **Interviewer Age (INTERVIEWER_AGE)**: Specifies the age of the interviewer, which is included to examine possible influences of interviewer demographics.
- **Interviewer Gender (INTERVIEWER_GENDER)**: Indicates the gender of the interviewer, recorded as *Male* or *Female*.

Certain variables, such as **COUNTRY**, **SOCIOECONOMIC STATUS**, and **HOUSEHOLD COMPOSITION**, were excluded due to high levels of missing data or lesser relevance to the primary focus of the study. Similarly, variables with extensive categories were simplified to binary or grouped formats to enhance clarity and focus on essential distinctions.

After filtering and processing, the final dataset consists of **9,743 observations** and **25 variables**, structured to support a clear and concise examination of the study’s research objectives.






# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```




# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```



\newpage


# References


